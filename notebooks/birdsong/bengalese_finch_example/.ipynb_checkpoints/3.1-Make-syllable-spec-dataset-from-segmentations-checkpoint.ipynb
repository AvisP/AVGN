{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook extracts syllables from a segmented waveform into spectrograms\n",
    "- This notebook takes WAV datasets generated by `1.0-segment-song-from-wavs` and segments generated by `2.1-Segment-syllables-make-textgrid`\n",
    "  - WAVs are expected to be in this format: `2017-04-16_17-27-44-760000.wav` and TextGrids are explected to be in the format `2017-04-16_17-27-44-760000.TextGrid`\n",
    "- The notebook outputs an HDF5 file which contains metadata about who the individual is, when the syllable was sung, how long the syllable is, which file the syllable comes from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.285Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.288Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import copy\n",
    "from praatio import tgio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.295Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.297Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from avgn.utils.audio import load_wav, int16_to_float32\n",
    "from avgn.utils.paths import DATA_DIR, ensure_dir\n",
    "from avgn.utils.general import save_dict_pickle, zero_one_norm, rescale\n",
    "from avgn.bout_segmentation.dynamic_threshold_segmentation import (\n",
    "    dynamic_spectrogram,\n",
    "    contiguous_regions,\n",
    "    cut_syllables,\n",
    "    boundaries_to_textgrid,\n",
    ")\n",
    "from avgn.visualization.spectrogram import (\n",
    "    plot_segmentations,\n",
    "    plot_bout_to_syllable_pipeline,\n",
    "    visualize_spec\n",
    ")\n",
    "from avgn.bout_segmentation.segment_wav_from_textgrid import extract_syllables\n",
    "from avgn.signalprocessing.spectrogramming import _build_mel_basis, inv_spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for segmenting syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.300Z"
    }
   },
   "outputs": [],
   "source": [
    "# the size of the syllables (pixels*pixels)\n",
    "syll_size = 32\n",
    "\n",
    "# parameters for filtering\n",
    "filtering_params = {\n",
    "    # filtering\n",
    "    \"highcut\": 15000,\n",
    "    \"lowcut\": 500,\n",
    "}\n",
    "\n",
    "mel_params = {\n",
    "    \"mel_filter\": False,  # should a mel filter be used?\n",
    "    \"num_mels\": syll_size,  # how many channels to use in the mel-spectrogram\n",
    "    \"fmin_mel\": 300,  # low frequency cutoff for mel filter\n",
    "    \"fmax_mel\": None,  # high frequency cutoff for mel filter\n",
    "    \"_mel_basis\": None,\n",
    "}\n",
    "\n",
    "spectrogramming_params = {\n",
    "    # spectrograms\n",
    "    \"num_freq\": 512,  # how many channels to use in a spectrogram\n",
    "    \"num_freq_final\": syll_size,  # how many channels to use in the resized spectrogram\n",
    "    \"preemphasis\": 0.97,\n",
    "    \"min_silence_for_spec\": 0.05,  # minimum length of silence for a spectrogram to be considered a good spectrogram\n",
    "    \"max_vocal_for_spec\": 5.0,  # the longest a single vocalization (protosyllable) is allowed to be\n",
    "    \"frame_shift_ms\": 0.5,  # step size for fft\n",
    "    \"frame_length_ms\": 6,  # frame length for fft\n",
    "    \"min_level_dB\": -100,  # minimum threshold db for computing spe\n",
    "    \"min_level_dB_floor\": -20,  # (db)\n",
    "    \"spec_thresh_delta_dB\": 5,  # (db) what\n",
    "    \"ref_level_dB\": 20,  # reference db for computing spec\n",
    "    \n",
    "}\n",
    "\n",
    "bout_threshold_params = {\n",
    "    # Silence Thresholding\n",
    "    \"silence_threshold\": 0.01,  # normalized threshold for silence\n",
    "    \"min_len\": 5.0,  # minimum length for a vocalization (fft frames)\n",
    "}\n",
    "\n",
    "syllabification_params = {\n",
    "    # Syllabification\n",
    "    \"min_syll_len_s\": 0.03,  # minimum length for a syllable\n",
    "    \"min_num_sylls\": 20,  # min number of syllables to be considered a bout\n",
    "    \"max_size_syll\": syll_size,  # the size of the syllable\n",
    "    \"resize_samp_fr\": int(\n",
    "        syll_size * 10.0\n",
    "    ),  # (frames/s) the framerate of the syllable (in compressed spectrogram time components)\n",
    "}\n",
    "spectrogram_inversion_params = {\n",
    "    # spectrogram inversion\n",
    "    \"griffin_lim_iters\": 60,\n",
    "    \"power\": 1.5,\n",
    "    # Thresholding out noise\n",
    "    \"mel_noise_filt\": 0.15,  # thresholds out low power noise in the spectrum - higher numbers will diminish inversion quality\n",
    "}\n",
    "\n",
    "hparams = {\"species\": \"BF\", \"dataset\": \"Koumura_Okanoya\"}\n",
    "\n",
    "for d in [\n",
    "    filtering_params,\n",
    "    spectrogramming_params,\n",
    "    bout_threshold_params,\n",
    "    syllabification_params,\n",
    "    spectrogram_inversion_params,\n",
    "    mel_params\n",
    "]:\n",
    "    for k, v in d.items():\n",
    "        hparams[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/Projects/github_repos/AVGN_419/AVGN/data/parameter_dictionaries/2019-04-30_14-18-12_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "# this is used to identify this training instance\n",
    "now_string = datetime.now().strftime(\n",
    "    \"%Y-%m-%d_%H-%M-%S\"\n",
    ")  \n",
    "# save the dictionary so that we can reload it for recovering waveforms\n",
    "dict_save = DATA_DIR / (\"parameter_dictionaries/\" + now_string + \"_dict.pickle\")\n",
    "ensure_dir(dict_save)\n",
    "save_dict_pickle(hparams, dict_save)\n",
    "print(dict_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.304Z"
    }
   },
   "outputs": [],
   "source": [
    "# build a basis function if you are using a mel spectrogram\n",
    "if hparams['mel_filter']:\n",
    "    hparams['_mel_basis'] = _build_mel_basis(hparams) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment bouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run through in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.312Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from avgn.bout_segmentation.dynamic_threshold_segmentation import textgrid_from_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/mnt/cube/tsainbur/Projects/github_repos/AVGN_419/AVGN/data/BF/Koumura_Okanoya/bouts/Bird9'),\n",
       " PosixPath('/mnt/cube/tsainbur/Projects/github_repos/AVGN_419/AVGN/data/BF/Koumura_Okanoya/bouts/Bird3'),\n",
       " PosixPath('/mnt/cube/tsainbur/Projects/github_repos/AVGN_419/AVGN/data/BF/Koumura_Okanoya/bouts/Bird4')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the data bird folders\n",
    "dataset_location = DATA_DIR / hparams['species'] / hparams['dataset'] / 'bouts'\n",
    "indv_folders = list(dataset_location.glob('*'))\n",
    "indv_folders[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.317Z"
    }
   },
   "outputs": [],
   "source": [
    "# skip creating datasets that already exist\n",
    "skip_existing = True \n",
    "\n",
    "# run through WAVs in parallel\n",
    "parallel = True \n",
    "verbosity = 10 # how verbose parallel should be\n",
    "n_jobs = 24 # how many jobs to run in parallel\n",
    "\n",
    "# visualize the output of the algorithm for optimizing parameters\n",
    "visualize = False \n",
    "\n",
    "# whether to save the dataset\n",
    "save_dataset=True \n",
    "\n",
    "# whether or not to output text for debugging\n",
    "verbose = False\n",
    "\n",
    "# visualization\n",
    "nex = 10 # how many example wavs to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/cube/tsainbur/Projects/github_repos/AVGN_419/AVGN/data/BF/Koumura_Okanoya/syllable_spectrograms/32')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllable_data_out = DATA_DIR / hparams['species'] / hparams['dataset'] / 'syllable_spectrograms' / str(syll_size)\n",
    "ensure_dir(syllable_data_out)\n",
    "syllable_data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.327Z"
    }
   },
   "outputs": [],
   "source": [
    "from avgn.bout_segmentation.segment_wav_from_textgrid import prepare_syllable_dataset\n",
    "import deepdish as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-30T21:18:08.333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ebc29d8b7b4da7882a9fba81f1648c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bird9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a90606272a4f898d19f9bf86c21644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=154), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:   17.9s\n"
     ]
    }
   ],
   "source": [
    "# loop through and make individual datasets\n",
    "for indv_folder in tqdm(indv_folders):\n",
    "\n",
    "    # get the birds name\n",
    "    indv_name = indv_folder.name\n",
    "    print(indv_name)\n",
    "\n",
    "    # check if the file already exists\n",
    "    dataset_loc = syllable_data_out / (indv_name + '.hdf5')\n",
    "    if dataset_loc.is_file() and skip_existing:\n",
    "        continue\n",
    "    \n",
    "    # get wav_list\n",
    "    wav_list = list((indv_folder / \"wavs\").glob(\"*.wav\"))\n",
    "    # get textgrid location\n",
    "    textgrid_loc = indv_folder / \"TextGrids\"\n",
    "    \n",
    "    # if visualizing, make sure only to show a few elements\n",
    "    if visualize == True:\n",
    "        wav_list = wav_list[:nex]\n",
    "\n",
    "    indv_data = prepare_syllable_dataset(\n",
    "        indv_name,\n",
    "        wav_list,\n",
    "        dataset_loc,\n",
    "        textgrid_loc,\n",
    "        hparams=hparams,\n",
    "        parallel=parallel,\n",
    "        n_jobs=n_jobs,\n",
    "        par_verbosity=verbosity,\n",
    "        verbose=verbose,\n",
    "        visualize=visualize,\n",
    "    )\n",
    "    # convert datetime to numpy datetime so that it can be saved as HDF5\n",
    "    indv_data['wav_datetime'] = [np.datetime64(i) for i in indv_data['wav_datetime']]\n",
    "    \n",
    "    # save the dataset as hdf5\n",
    "    dd.io.save(dataset_loc, indv_data, compression=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
