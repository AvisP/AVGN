{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.220Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.228Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.231Z"
    }
   },
   "outputs": [],
   "source": [
    "from avgn.networks.gaia2 import GAIA\n",
    "from avgn.networks.test_datasets import load_fashion_MNIST\n",
    "from avgn.networks.sample_networks import fc_net, conv_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.233Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "latent_dim = 128\n",
    "batch_size = 128\n",
    "dims = (28 , 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.235Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    RepeatVector,\n",
    "    Dense,\n",
    "    TimeDistributed,\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    Reshape,\n",
    "    Bidirectional,\n",
    "    concatenate,\n",
    "    Input,\n",
    "    UpSampling2D,\n",
    "    MaxPooling2D,\n",
    ")  # , LSTM\n",
    "from tensorflow.python.keras.layers.recurrent import UnifiedLSTM as LSTM\n",
    "from tensorflow.python.keras.layers.recurrent import UnifiedGRU as GRU\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.237Z"
    }
   },
   "outputs": [],
   "source": [
    "def unet_convblock_down(\n",
    "    _input,\n",
    "    channels=16,\n",
    "    kernel=(3, 3),\n",
    "    activation=\"relu\",\n",
    "    pool_size=(2, 2),\n",
    "    kernel_initializer=\"he_normal\",\n",
    "):\n",
    "    conv = Conv2D(\n",
    "        channels,\n",
    "        kernel,\n",
    "        activation=activation,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(_input)\n",
    "    conv = Conv2D(\n",
    "        channels,\n",
    "        kernel,\n",
    "        activation=activation,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(conv)\n",
    "    pool = MaxPooling2D(pool_size=pool_size)(conv)\n",
    "    return conv, pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.239Z"
    }
   },
   "outputs": [],
   "source": [
    "def unet_convblock_up(\n",
    "    last_conv,\n",
    "    cross_conv,\n",
    "    channels=16,\n",
    "    kernel=(3, 3),\n",
    "    activation=\"relu\",\n",
    "    pool_size=(2, 2),\n",
    "    kernel_initializer=\"he_normal\",\n",
    "):\n",
    "    \n",
    "    up_conv = UpSampling2D(size=(2, 2))(last_conv)\n",
    "    merge = concatenate([up_conv, cross_conv], axis = 3)\n",
    "    conv = Conv2D(\n",
    "        channels,\n",
    "        kernel,\n",
    "        activation=activation,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(merge)\n",
    "    conv = Conv2D(\n",
    "        channels,\n",
    "        kernel,\n",
    "        activation=activation,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.242Z"
    }
   },
   "outputs": [],
   "source": [
    "def unet_mnist():\n",
    "    inputs = Input(shape=(28, 28, 1))\n",
    "    up_1, pool_1 = unet_convblock_down(inputs, channels=32)\n",
    "    up_2, pool_2 = unet_convblock_down(pool_1, channels=64)\n",
    "    conv_middle = Conv2D(\n",
    "        128, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\"\n",
    "    )(pool_2)\n",
    "    conv_middle = Conv2D(\n",
    "        128, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\"\n",
    "    )(conv_middle)\n",
    "    down_2 = unet_convblock_up(conv_middle, up_2, channels=64)\n",
    "    down_1 = unet_convblock_up(down_2, up_1, channels=32)\n",
    "    outputs = Conv2D(1, (1,1), activation=\"sigmoid\")(down_1)\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.244Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_recon(example_data, nex = 3, zm = 2):\n",
    "    z, xg, zi, xi, d_xi, d_x, d_xg = model.network_pass(example_data)\n",
    "    fig, axs = plt.subplots(ncols=6, nrows = nex, figsize=(zm*6, zm*nex))\n",
    "    for axi, (dat, lab) in enumerate(\n",
    "        zip(\n",
    "            [example_data, d_x, xg, d_xg, xi, d_xi],\n",
    "            [\"data\", \"disc data\",\"gen\", \"disc gen\", \"interp\", \"disc interp\"],\n",
    "        )\n",
    "    ):\n",
    "        for ex in range(nex):\n",
    "            axs[ex, axi].matshow(dat.numpy()[ex].squeeze(), cmap = plt.cm.Greys, vmin=0, vmax=1)\n",
    "            axs[ex, axi].axis('off')\n",
    "        axs[0, axi].set_title(lab)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.246Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    fig, axs = plt.subplots(ncols=5, figsize=(15,4))\n",
    "    x = np.arange(len(losses))\n",
    "    axs[0].semilogx(losses.D_prop.values, label='D_prop')\n",
    "    axs[1].loglog(x, losses.d_xg_loss.values, label='d_xg_loss')\n",
    "    axs[2].loglog(x, losses.d_xi_loss.values, label='d_xi_loss')\n",
    "    axs[3].loglog(x, losses.d_x_loss.values, label='d_x_loss')\n",
    "    axs[4].loglog(x, losses.xg_loss.values, label='xg_loss')\n",
    "    for ax in axs.flatten():\n",
    "        ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.250Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_losses(model, test_dataset, batch_size=512, TEST_BUF=1000):\n",
    "    n_test = int(np.sum([True for i in test_dataset])*.9)\n",
    "    test_loss = []\n",
    "    tx = 0\n",
    "    for test_x in tqdm(test_dataset, total=n_test, leave=False):\n",
    "        test_loss.append(model.compute_loss(test_x))\n",
    "        tx += 1\n",
    "        if tx >=n_test:\n",
    "            break\n",
    "    D_prop,d_xg_loss, d_xi_loss, d_x_loss, xg_loss = np.mean(\n",
    "        [[i.numpy() for i in row] for row in test_loss], axis=0\n",
    "    )\n",
    "    return D_prop, d_xg_loss, d_xi_loss, d_x_loss, xg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.252Z"
    }
   },
   "outputs": [],
   "source": [
    "network_arch = conv_net(dims=dims, n_Z = latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.254Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get the datasets\n",
    "TRAIN_BUF = 60000\n",
    "train_dataset, test_dataset = load_fashion_MNIST(BATCH_SIZE=batch_size, TRAIN_BUF=TRAIN_BUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.255Z"
    }
   },
   "outputs": [],
   "source": [
    "from avgn.networks.gaia4 import GAIA\n",
    "\n",
    "# prepare the optimizer\n",
    "lrbase = 1e-3 # 1e-3\n",
    "gen_optimizer = tf.keras.optimizers.Adam(lrbase, beta_1 = 0.5)\n",
    "disc_optimizer = tf.keras.optimizers.RMSprop(lrbase)\n",
    "# train the model\n",
    "model = GAIA(\n",
    "    enc = network_arch.encoder,\n",
    "    dec = network_arch.decoder,\n",
    "    unet_function = unet_mnist,\n",
    "    dims = dims, \n",
    "    gen_optimizer = gen_optimizer,\n",
    "    disc_optimizer = disc_optimizer,\n",
    "    batch_size = batch_size,\n",
    "    alpha = 0.0, #0.25\n",
    "    lr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.257Z"
    }
   },
   "outputs": [],
   "source": [
    "example_data = next(iter(train_dataset))\n",
    "z = model.encode(example_data)\n",
    "zi = model._interpolate_z(z)\n",
    "d_x = model.discriminate(example_data)\n",
    "#plot_recon(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.260Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(columns=['D_prop', 'd_xg_loss', 'd_xi_loss', 'd_x_loss', 'xg_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.264Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1000): \n",
    "    txi = 0\n",
    "    for train_x in tqdm(train_dataset):\n",
    "        if txi == int(TRAIN_BUF/batch_size)-1: continue\n",
    "        try:\n",
    "            model.train(train_x)\n",
    "        except ValueError: \n",
    "            continue\n",
    "        txi +=1\n",
    "        \n",
    "        #if txi % 100 == 0:\n",
    "        #    print(\"Epoch {}\".format(epoch))\n",
    "        #    plot_recon(example_data)\n",
    "        #    losses.loc[len(losses)] = test_losses(model, test_dataset, batch_size=batch_size)\n",
    "        #    plot_losses(losses)\n",
    "        #    plt.show()\n",
    "    display.clear_output(wait=False)\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    plot_recon(example_data)\n",
    "    losses.loc[len(losses)] = test_losses(model, test_dataset, batch_size=batch_size)\n",
    "    plot_losses(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.266Z"
    }
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-09T04:27:34.268Z"
    }
   },
   "outputs": [],
   "source": [
    "nx = 3000\n",
    "fix, axs = plt.subplots(ncols = 4, figsize=(16,4))\n",
    "for i, npts in enumerate([1, 10, 100, 100]):\n",
    "    z = tf.random.normal(shape=(npts,nx))\n",
    "    ip = tf.convert_to_tensor(\n",
    "        #np.random.vonmises(size=(z.shape[0], z.shape[0]), mu = 1, kappa = 1),\n",
    "        #np.random.uniform(size=(z.shape[0], z.shape[0])),\n",
    "        np.random.chisquare(1/npts, size=(z.shape[0], z.shape[0])),\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    ip = ip / tf.reduce_sum(ip, axis=0)\n",
    "    zi = tf.transpose(tf.tensordot(tf.transpose(z), ip, axes=1))\n",
    "    axs[i].hist(z.numpy().flatten(), alpha = 0.5);\n",
    "    axs[i].hist(zi.numpy().flatten(), alpha = 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
